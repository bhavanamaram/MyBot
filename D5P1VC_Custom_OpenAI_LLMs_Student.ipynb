{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bhavanamaram/MyBot/blob/main/D5P1VC_Custom_OpenAI_LLMs_Student.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step-by-Step Guide ChatGPT Python API\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "RoAimVPu0MIT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Harvard AI Bootcamp"
      ],
      "metadata": {
        "id": "8lkCw2PY_MQS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Make a copy of this notebook! Editing directly will not be saved."
      ],
      "metadata": {
        "id": "RLqCg9zF_NZE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imports"
      ],
      "metadata": {
        "id": "0qI4qQLM00nm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Lb0DWDkS7ZZK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade openai"
      ],
      "metadata": {
        "id": "00X1OkvG1BFu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "After you have installed it just import it as shown below."
      ],
      "metadata": {
        "id": "iW5gSJtr1JtZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from openai import OpenAI\n",
        "!pip install cohere tiktoken"
      ],
      "metadata": {
        "id": "GYcn9r8f0LI7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can access our API key here: https://docs.google.com/document/d/1BGBcBFckMOhxsTT8fLRrcwaE4kOBWcTDgqC62GIRR8k/edit?usp=sharing"
      ],
      "metadata": {
        "id": "lPRqx_oE1UHO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "OPENAI_API_KEY = ''\n",
        "client = OpenAI(api_key=OPENAI_API_KEY)"
      ],
      "metadata": {
        "id": "mvJtWsiFIi6E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Embedding The OpenAI ChatGPT API In A JupyterÂ Notebook"
      ],
      "metadata": {
        "id": "nZrz0Mx12FSr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Having set up the mandatory openai library and API key, we can now continue by making a test request to check whether everything is working properly.\n",
        "\n",
        "Before being able to retrieve a response, we have to define the model first. In our case we would like to utilize the **gpt-3.5-turbo** model.\n",
        "\n",
        "If you want to get a list of all models than just execute the code cell below"
      ],
      "metadata": {
        "id": "I7VOW3hd2G6Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for model in client.models.list():\n",
        "    print(model.id)"
      ],
      "metadata": {
        "id": "rjMZX1mL32ts"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define A message"
      ],
      "metadata": {
        "id": "sIHvB9aN6lb5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Crafting a message requires to steps:\n",
        "\n",
        "1. Define the **role**\n",
        "2. Define the **content**\n",
        "\n",
        "The **role** can be one of three things: *system*, *user*, *assistant*\n",
        "\n",
        "A short explanation:\n",
        "\n",
        " - *system*: Primes the chatbot to act in a specific way, e.g., 'act as a helpful mathematics professor'\n",
        " - *user*: This is basically *you*, and is used for all queries you make\n",
        " - *assistant*: This is a previous response of the chatbot, useful for conversations in which you might refer to the previous outputs"
      ],
      "metadata": {
        "id": "2gDBvG726pzv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For our first example, we just want to query a response by asksing in the role of the *user*."
      ],
      "metadata": {
        "id": "aFD254Xs8dva"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = \"gpt-4-0314\""
      ],
      "metadata": {
        "id": "tv8C7VCbAew2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "my_first_message = \"How many people live in Brasil?\"\n",
        "\n",
        "# TODO\n",
        "# Define the role as user and content of the prompt\n",
        "message_list = [\n",
        "    {\n",
        "        \"role\": ,\n",
        "        \"content\":\n",
        "    }\n",
        "]"
      ],
      "metadata": {
        "id": "SsaEaRkv6ocJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Get The Response"
      ],
      "metadata": {
        "id": "By0oXmgEKp42"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "completion = client.chat.completions.create(model=model, messages=message_list)"
      ],
      "metadata": {
        "id": "rXX0DjJO844i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since we received no error message the query should have been successful. The complete response looks as follows:"
      ],
      "metadata": {
        "id": "hCLtgjuw9Vzg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "completion"
      ],
      "metadata": {
        "id": "iAHo86LW8_pY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "However, we are mostly interested into the answer we asked for. Retrieve this message from the output directionary."
      ],
      "metadata": {
        "id": "7sJkW3au9ZbV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO\n",
        "# Store the response string in the \"response\" variable\n",
        "response =\n",
        "print(response)"
      ],
      "metadata": {
        "id": "R1VIzjhh3S8i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can also enter text input right in your notebook with the input() function! Look at the function below for an example of how to do this."
      ],
      "metadata": {
        "id": "2nrC7RkUme06"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO\n",
        "# Define the message list as user and input prompt\n",
        "def get_response():\n",
        "  prompt = input(\"Enter prompt: \")\n",
        "  message_list = [\n",
        "    {\n",
        "        \"role\": ,\n",
        "        \"content\":\n",
        "    }\n",
        "  ]\n",
        "  completion = client.chat.completions.create(model=model, messages=message_list)\n",
        "  response =\n",
        "  return response"
      ],
      "metadata": {
        "id": "4L200IfqmeM8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "get_response()"
      ],
      "metadata": {
        "id": "ITQqGll3m-as"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Note:** As you may have noticed we receive some additional valuable information in the response as well:\n",
        "\n",
        " - **role**: assistant (aka. the chatbot)\n",
        " - **model**: gpt-4-0613 (the specific model model name/id)\n",
        " - **usage**: a dictionary (relevant for calcualting the price of your queries):\n",
        "    - *prompt_tokens*: the amount of tokens used in your query\n",
        "    - *completion_tokens*: the amount of tokens used in the response\n",
        "    - *total_token*: the total number of tokens"
      ],
      "metadata": {
        "id": "6xSAst079x2Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "And that is it for the start.\n",
        "\n",
        "Below is more sophisticated code that allows you to have a real chat-like conversation in your Jupyter Notebook or an application of your choice."
      ],
      "metadata": {
        "id": "uo0ESyc3_Lc_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setting up a Chat"
      ],
      "metadata": {
        "id": "uh-wkNJI_X6Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Setting up a chat actually won't require much extra work.\n",
        "\n",
        "What we basically need to add is the option to provide the whole chat history as a message in order for the model to understand and know the whole context.\n",
        "\n",
        "To do this I will use the *chat_history* variable - define below - to keep track of all the queries I made and responses I have received. Each time I ask something new or get a response I will append that content with the proper **role** to the list as a dictionary."
      ],
      "metadata": {
        "id": "hRmKtoJRS9hP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chat_history = []"
      ],
      "metadata": {
        "id": "FWwoydnyS84m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def chat(user_input: str, chat_history: list = [], role: str = \"user\", model: str = \"gpt-3.5-turbo\") -> list:\n",
        "    # add your new query to the chat history\n",
        "    chat_history.append(\n",
        "        {\n",
        "            \"role\": role,\n",
        "            \"content\": user_input\n",
        "        }\n",
        "    )\n",
        "\n",
        "    # post a request to the openai API\n",
        "    completion = client.chat.completions.create(\n",
        "        model=model,\n",
        "        messages=chat_history)\n",
        "\n",
        "    # get the response\n",
        "    response = completion.choices[0].message.content\n",
        "\n",
        "    # append the response with the role - assistant - to the chat history\n",
        "    chat_history.append(\n",
        "        {\n",
        "            \"role\": completion.choices[0].message.role,\n",
        "            \"content\": response\n",
        "        }\n",
        "    )\n",
        "\n",
        "    # return the response and chat_history\n",
        "    return response, chat_history"
      ],
      "metadata": {
        "id": "UyoSFebY3S5S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Start chatting"
      ],
      "metadata": {
        "id": "pAA6cWPgZQYn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Prime the model with a system message"
      ],
      "metadata": {
        "id": "kofIzJt0ZXiW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Having defined the chat function, I would test it with an example making asking two consecutive questions where the second relies on the context of the first and its response.\n",
        "\n",
        "In addition, we will prime the model by using the *system* role, to hopefully get a better response."
      ],
      "metadata": {
        "id": "PcihMA17VUKC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# setting the role to system to prime the model\n",
        "response, chat_history = chat(\n",
        "    user_input = \"Act as a helpful math teacher.\",\n",
        "    chat_history = chat_history,\n",
        "    role = \"system\"\n",
        ")"
      ],
      "metadata": {
        "id": "Jpdw59nZ3S1z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's print the response and chat history to see if everything worked as expected."
      ],
      "metadata": {
        "id": "Baa9d3pEWWyB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(response)"
      ],
      "metadata": {
        "id": "-ZlxAdmX3SXl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chat_history"
      ],
      "metadata": {
        "id": "UB9WVXdgWUaw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### My Initial Question"
      ],
      "metadata": {
        "id": "4K24yFLkZb71"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The response as well as the chat_history looks as we wanted it, let's see if the AI model behaves as it was advised to."
      ],
      "metadata": {
        "id": "AL9xp934Wb1b"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "84xyKPeuMvwm"
      },
      "outputs": [],
      "source": [
        "response, chat_history = chat(\n",
        "    user_input = input(\"Ask me a math question (e.g. derivative of the function f(x) = x * e^x)\"),\n",
        "    chat_history = chat_history\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(response)"
      ],
      "metadata": {
        "id": "ItofzPyoW2VG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Follow-Up Question"
      ],
      "metadata": {
        "id": "Xc2d4UUJZio_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's see if it can answer my follow-up question which will require an understanding of the context of the previous question and response."
      ],
      "metadata": {
        "id": "VwECID62ZilS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "response, chat_history = chat(\n",
        "    user_input = input(\"Ask me to reverse the previous calculation! \"),\n",
        "    chat_history = chat_history\n",
        ")"
      ],
      "metadata": {
        "id": "4vdabEQlW4cT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(response)"
      ],
      "metadata": {
        "id": "zJvJTp2fXlsl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, let's look at the complete chat history:"
      ],
      "metadata": {
        "id": "wTTScTpZcrxx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for el in chat_history:\n",
        "    str_content = el[\"content\"].replace(\"\\n\", \"\\n            | \")\n",
        "    print(f'{el[\"role\"].capitalize()}:{\" \" * (11 - len(el[\"role\"]))}| {str_content}\\n')"
      ],
      "metadata": {
        "id": "o2CvM3V9YwHR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Try debugging these examples to test your Open AI API knowledge!"
      ],
      "metadata": {
        "id": "HhXIoz2V6WgP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"True or false: a banana is smaller than a lemon.\\n\\n\"\n",
        "\n",
        "response = client.completions.create(\n",
        "    prompt=prompt,\n",
        "    model=\"gpt-3.5-turbo-instruct\",\n",
        "    top_p=0.5, max_tokens=50,\n",
        "    stream=True)\n",
        "for part in response:\n",
        "    print(part.choices[0].text or \"\")"
      ],
      "metadata": {
        "id": "lBmdJJzY6fja"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note that there are two endpoints: chat.completions and completions. Chat.completions corresponds to models which are meant to give completions for chat dialogues (e.g. gpt-3.5-turbo) while completions are meant to give a completion to a single string of text input (so there is no messages/dialogue input for prompting). Most models listed are completion models, while the gpt-3.5 and gpt-4 models are chat completion models."
      ],
      "metadata": {
        "id": "2tIDhwCF6hMA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a chat completion\n",
        "response = client.chat.completions.create(\n",
        "    model=\"gpt-4-0314\",  # You can choose the model you want to use\n",
        "    messages=[\n",
        "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "        {\"role\": \"user\", \"content\": \"Translate 'Hello, how are you?' to French.\"}\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Print the response\n",
        "print(response.choices[0].message.content)"
      ],
      "metadata": {
        "id": "3fsF681e6o83"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Testing Different GPT Models"
      ],
      "metadata": {
        "id": "n3PeWoKQ7D16"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "How about davinci instead of GPT 4?"
      ],
      "metadata": {
        "id": "AAuDqMfDsVXv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"Translate 'Hello, how are you?' to French.\"\n",
        "# TODO\n",
        "# Retrieve a response from the Davinci model with the prompt\n",
        "response = client.completions.create(model= ,\n",
        "  prompt=,\n",
        "  temperature=0.7,\n",
        "  max_tokens=150)\n",
        "print(response.choices[0].text.strip())"
      ],
      "metadata": {
        "id": "wemg0yzqsM_-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = client.chat.completions.create(\n",
        "    # TODO\n",
        "    # Select a GPT-4 Model\n",
        "    model=,\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": \"You are a very unhelpful assistant.\"},\n",
        "        {\"role\": \"user\", \"content\": \"Translate the following English text to French: 'Hello, how are you?'\"}\n",
        "    ])\n",
        "\n",
        "print(response.choices[0].message.content)"
      ],
      "metadata": {
        "id": "MQkYu6kt6s10"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "What if today is opposite day?"
      ],
      "metadata": {
        "id": "pK3R3rEzDv1P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "response = client.chat.completions.create(\n",
        "    model=\"gpt-4-0314\",\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": \"You are a very helpful assistant, *BUT* today is oppposite day.\"},\n",
        "        {\"role\": \"user\", \"content\": \"Translate the following English text to French: 'Hello, how are you?'\"}\n",
        "    ])\n",
        "\n",
        "print(response.choices[0].message.content)"
      ],
      "metadata": {
        "id": "082Ygs9hr5qR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "AI Chef"
      ],
      "metadata": {
        "id": "goh1kKqmLuFm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"I have chicken, rice, and broccoli. What can I cook with these ingredients?\"\n",
        "response = client.completions.create(model=\"text-davinci-003\",\n",
        "  prompt=prompt,\n",
        "  temperature=0.7,\n",
        "  max_tokens=150)\n",
        "\n",
        "print(response.choices[0].text.strip())"
      ],
      "metadata": {
        "id": "fdtVkj6F6-IU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "AI Musician"
      ],
      "metadata": {
        "id": "RKIi_0G_7Hqy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"Create a short melody in the key of C major. Make the output format like 'A B D G'\"\n",
        "response = client.completions.create(model=\"text-davinci-003\",  # Replace with your preferred model\n",
        "prompt=prompt,\n",
        "temperature=0.7,\n",
        "max_tokens=100)\n",
        "\n",
        "note_series = response.choices[0].text.strip()\n",
        "print(note_series)"
      ],
      "metadata": {
        "id": "_JMnWEYO7ABm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "What does it sound like?"
      ],
      "metadata": {
        "id": "zMT4jHCjt5-H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from IPython.display import Audio\n",
        "\n",
        "# Define the musical notes and their corresponding frequencies (in Hertz)\n",
        "note_frequencies = {\n",
        "    \"C\": 261.63,\n",
        "    \"D\": 293.66,\n",
        "    \"E\": 329.63,\n",
        "    \"F\": 349.23,\n",
        "    \"G\": 392.00,\n",
        "    \"A\": 440.00,\n",
        "    \"B\": 493.88\n",
        "}\n",
        "\n",
        "# Split the notes into a list\n",
        "# Ode to joy: note_series = \"E E E F G G F E D C C D E E D D\"\n",
        "notes = note_series.split()\n",
        "\n",
        "# Define the sample rate and duration per note\n",
        "sample_rate = 44100  # Hz\n",
        "duration_per_note = 0.5  # seconds\n",
        "\n",
        "# Function to generate a sine wave for a given frequency and duration\n",
        "def generate_sine_wave(freq, duration, sample_rate):\n",
        "    t = np.linspace(0, duration, int(sample_rate * duration), False)\n",
        "    return np.sin(freq * 2 * np.pi * t)\n",
        "\n",
        "# Generate the sine wave for each note\n",
        "audio_wave = np.array([], dtype=np.float32)\n",
        "for note in notes:\n",
        "    frequency = note_frequencies.get(note, 0)  # Get the frequency of the note, default to 0 Hz\n",
        "    wave = generate_sine_wave(frequency, duration_per_note, sample_rate)\n",
        "    audio_wave = np.concatenate((audio_wave, wave))\n",
        "\n",
        "# Normalize the wave to 16-bit range\n",
        "audio_wave = (audio_wave * (2**15 - 1)).astype(np.int16)\n",
        "\n",
        "# Play the audio\n",
        "Audio(audio_wave, rate=sample_rate)\n"
      ],
      "metadata": {
        "id": "gJ0wsEQQsrav"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's duplicate the same response over many different models!"
      ],
      "metadata": {
        "id": "duWDFzISuK-n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "models = [\n",
        "    'gpt-3.5-turbo',\n",
        "    'gpt-3.5-turbo-1106',\n",
        "    'gpt-4-0314',\n",
        "    'gpt-4-1106-preview',\n",
        "]"
      ],
      "metadata": {
        "id": "r1uKLUy0uPKw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = input(\"Enter prompt: \")\n",
        "for model in models:\n",
        "  # TODO\n",
        "  # Get and print response for each model and print the output\n",
        "  response =\n",
        "  print(model)\n",
        "  print() # print response text\n",
        "  print()"
      ],
      "metadata": {
        "id": "84T6yFL5uqCz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's make an unreliable chatbot!"
      ],
      "metadata": {
        "id": "OH4uRReYqqo7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "system_prompts = [\n",
        "    'You are a very helpful assistant',\n",
        "    'You are an angry assistant',\n",
        "    'You are a very unhelpful assistant',\n",
        "    'You are an evil (but ethical) assistant',\n",
        "    'You are an alien that does not speak human',\n",
        "    'You are President John F. Kennedy',\n",
        "    'You are a cat',\n",
        "    # Feel free to add or edit these prompts\n",
        "]"
      ],
      "metadata": {
        "id": "VpwmYRXywOXq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "# TODO\n",
        "system_prompt = # Get a random system prompt\n",
        "\n",
        "# Get a response, inputting the selected system prompt and prompting user input with the input() function.\n",
        "# Use any model you like\n",
        "response =\n",
        "\n",
        "# Print the response and the system prompt used"
      ],
      "metadata": {
        "id": "hDiH6bAoquOd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Tk6OwsVo0XzF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}